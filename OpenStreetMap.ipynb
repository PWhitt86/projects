{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39412315",
   "metadata": {},
   "source": [
    "# Analysis of OpenStreetMap Data.\n",
    "\n",
    "Pittsburgh, PA, United States\n",
    "\n",
    "I have lived in Pittsburgh for the past 4 tears and I am interested in what the OSM will bring up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03468be6",
   "metadata": {},
   "source": [
    "# Following Code is From Project Details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a868c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "\n",
    "OSM_FILE = \"Pittsburgh.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"sample.osm\"\n",
    "\n",
    "k = 10 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7134d5",
   "metadata": {},
   "source": [
    "# The Code Below is to Find Out How Many Types of Tags There arte and the Number of Each Tag.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1679f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 69288,\n",
      " 'meta': 1,\n",
      " 'nd': 2252826,\n",
      " 'node': 1860250,\n",
      " 'note': 1,\n",
      " 'osm': 1,\n",
      " 'relation': 2765,\n",
      " 'tag': 820435,\n",
      " 'way': 323896}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "filename = \"Pittsburgh.osm\"\n",
    "# filename = \"Sample.osm\" #This is here so I can switch back and forth between the full .osm file and the sample\n",
    "\n",
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    for event, element in ET.iterparse(filename):\n",
    "        if element.tag not in tags.keys():\n",
    "            tags[element.tag] = 1\n",
    "        else:\n",
    "            tags[element.tag] += 1\n",
    "    return tags\n",
    "\n",
    "def test():\n",
    "\n",
    "    tags = count_tags('Pittsburgh.osm')\n",
    "    # tags = count_tags('Sample.osm') #This is here so I can switch back and forth between the full .osm file and the sample\n",
    "    pprint.pprint(tags)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae9b29",
   "metadata": {},
   "source": [
    "# The Code Below Allows You to Check the k Value for Each Tag by Classifying the Tags into Few Categories:\n",
    "1. \"lower\": valid tags containing only lowercase letters\n",
    "2. \"lower_colon\": valid tags with a colon in the names\n",
    "3. \"problemchars\": tags with problematic characters\n",
    "4. \"other\": other tags that don't fall into the 3 categories above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "babcf823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 543151, 'lower_colon': 269335, 'other': 7947, 'problemchars': 2}\n"
     ]
    }
   ],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        k = element.attrib['k']\n",
    "        if re.search(lower,k):\n",
    "            keys[\"lower\"] += 1\n",
    "        elif re.search(lower_colon,k):\n",
    "            keys[\"lower_colon\"] += 1\n",
    "        elif re.search(problemchars,k):\n",
    "            keys[\"problemchars\"] += 1\n",
    "        else:\n",
    "            keys[\"other\"] += 1\n",
    "    return keys\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "    return keys\n",
    "\n",
    "def test():\n",
    "    keys = process_map('Pittsburgh.osm')\n",
    "    # keys = process_map('Sample.osm') #This is here so I can switch back and forth between the full .osm file and the sample\n",
    "    pprint.pprint(keys)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76928530",
   "metadata": {},
   "source": [
    "# Code Below Lists Street Types Not in the Expected List."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de4f2b30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'22': set(['Business Route 22']),\n",
      " '48': set(['48']),\n",
      " 'Alley': set(['Clay Alley',\n",
      "               'Delmar Alley',\n",
      "               'McGuinness Alley',\n",
      "               'Oak Alley',\n",
      "               'Park Alley',\n",
      "               'Vine Alley']),\n",
      " 'Allies': set(['Boulevard of the Allies']),\n",
      " 'Automotive': set(['California Automotive']),\n",
      " 'Ave': set(['1st Ave',\n",
      "             '5th Ave',\n",
      "             'Arlington Ave',\n",
      "             'Centre Ave',\n",
      "             'E Warrington Ave',\n",
      "             'Elizabeth Ave',\n",
      "             'Fifth Ave',\n",
      "             'Forbes Ave',\n",
      "             'Friendship Ave',\n",
      "             'Greydon Ave',\n",
      "             'Highland Ave',\n",
      "             'Liberty Ave',\n",
      "             'Lynnwood Ave',\n",
      "             'Morewood Ave',\n",
      "             'Penn Ave',\n",
      "             'S Millvale Ave',\n",
      "             'S Negley Ave',\n",
      "             'S. Aiken Ave',\n",
      "             'Shadeland Ave',\n",
      "             'Shady Ave',\n",
      "             'South Aiken Ave',\n",
      "             'W Liberty Ave']),\n",
      " 'Ave.': set(['4th Ave.', 'Chartiers Ave.', 'Murray Ave.']),\n",
      " 'Bldg': set(['Windgap Ave Bldg']),\n",
      " 'Blvd': set(['Beechwood Blvd',\n",
      "              'Clairton Blvd',\n",
      "              'Fort Duquesne Blvd',\n",
      "              'Pennsbury Blvd',\n",
      "              'Washington Blvd']),\n",
      " 'Brdg': set(['Swindell Brdg']),\n",
      " 'Center': set(['Crafton Ingram Shopping Center']),\n",
      " 'Dr': set(['Clubhouse Dr', 'Eastminster Dr', 'Selvin Dr']),\n",
      " 'East': set(['Waterfront Drive East']),\n",
      " 'Expressway': set(['Tri-Boro Expressway']),\n",
      " 'Ext': set(['Grant Ave Ext']),\n",
      " 'Extended': set(['Aiken Road Extended',\n",
      "                  'Chartiers Avenue Extended',\n",
      "                  'Long Valley Drive Extended',\n",
      "                  'Maple Street Extended',\n",
      "                  'Montour Street Extended',\n",
      "                  'Thomas Street Extended']),\n",
      " 'Extension': set(['Broadway Avenue Extension',\n",
      "                   'Federal Street Extension',\n",
      "                   'Mount Troy Road Extension',\n",
      "                   'Patton Street Extension',\n",
      "                   'Thomas Street Extension',\n",
      "                   'Virginia Avenue Extension']),\n",
      " 'Harding': set(['Harding']),\n",
      " 'Highway': set(['Lincoln Highway', 'Perry Highway', 'William Penn Highway']),\n",
      " 'Kennedy': set(['Kennedy']),\n",
      " 'Mall': set(['Monroeville Mall']),\n",
      " 'Maples': set(['The Maples']),\n",
      " 'North': set(['Grandview Drive North']),\n",
      " 'Oaks': set(['The Oaks']),\n",
      " 'Park': set(['Arlington Park']),\n",
      " 'Patricia': set(['Patricia']),\n",
      " 'Pike': set(['Greensburg Pike',\n",
      "              'Kittanning Pike',\n",
      "              'Old Washington Pike',\n",
      "              'Steubenville Pike',\n",
      "              'Washington Pike']),\n",
      " 'Pl': set(['Montgomery Pl', 'Washington Pl']),\n",
      " 'Plaza': set(['Mckees Rocks Plaza', 'Penn Plaza']),\n",
      " 'Rd': set(['520 Unity Center Rd',\n",
      "            'Bayard Rd',\n",
      "            'Browns Hill Rd',\n",
      "            'Brownsville Rd',\n",
      "            'Duff Rd',\n",
      "            'Jefferson Rd',\n",
      "            'Library Rd',\n",
      "            'McNeilly Rd']),\n",
      " 'Row': set(['Coles Row']),\n",
      " 'South': set(['Grandview Drive South', 'Penn Circle South']),\n",
      " 'Sq': set(['Elmer L Williams Sq', 'Harvard Sq']),\n",
      " 'St': set(['7th St',\n",
      "            'Aster St',\n",
      "            'Bellefonte St',\n",
      "            'Castleman St',\n",
      "            'Copeland St',\n",
      "            'Hemlock St',\n",
      "            'Henry St',\n",
      "            'James St',\n",
      "            'Mary St',\n",
      "            'Mirror St',\n",
      "            'N Craig St',\n",
      "            'N Neville St',\n",
      "            'North Bellefield St',\n",
      "            'Railroad St',\n",
      "            'S 22nd St',\n",
      "            'S Craig St',\n",
      "            'S Graham St',\n",
      "            'S Whitfield St',\n",
      "            'Saline St',\n",
      "            'South Craig St',\n",
      "            'South Dithridge St',\n",
      "            'Stanwix St',\n",
      "            'Walnut St',\n",
      "            'West St',\n",
      "            'Winthrop St']),\n",
      " 'St.': set(['Atwood St.', 'Bryant St.', 'Byrant St.', 'West 11th St.']),\n",
      " 'Ter': set(['Colby Ter', 'Faber Ter']),\n",
      " 'West': set(['Waterfront Drive West']),\n",
      " 'howe': set(['howe'])}\n"
     ]
    }
   ],
   "source": [
    "OSMFILE = \"Pittsburgh.osm\"\n",
    "# OSMFILE = \"sample.osm\" #This is here so I can switch back and forth between the full .osm file and the sample\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Circle\", \"Terrace\", \"Way\"]\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "st_types = audit(OSMFILE)\n",
    "\n",
    "def test():    \n",
    "    pprint.pprint(dict(st_types))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c5d9c",
   "metadata": {},
   "source": [
    "# Code updates the unexpected streets listed in the mapping list while keeping other streets unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d712a9c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Waterfront Drive West', '=>', 'Waterfront Drive West')\n",
      "('Byrant St.', '=>', 'Byrant Street')\n",
      "('Bryant St.', '=>', 'Bryant Street')\n",
      "('West 11th St.', '=>', 'West 11th Street')\n",
      "('Atwood St.', '=>', 'Atwood Street')\n",
      "('Swindell Brdg', '=>', 'Swindell Brdg')\n",
      "('Patricia', '=>', 'Patricia')\n",
      "('Browns Hill Rd', '=>', 'Browns Hill Road')\n",
      "('Jefferson Rd', '=>', 'Jefferson Road')\n",
      "('Bayard Rd', '=>', 'Bayard Road')\n",
      "('Library Rd', '=>', 'Library Road')\n",
      "('520 Unity Center Rd', '=>', '520 Unity Center Road')\n",
      "('Brownsville Rd', '=>', 'Brownsville Road')\n",
      "('Duff Rd', '=>', 'Duff Road')\n",
      "('McNeilly Rd', '=>', 'McNeilly Road')\n",
      "('Boulevard of the Allies', '=>', 'Boulevard of the Allies')\n",
      "('The Oaks', '=>', 'The Oaks')\n",
      "('Waterfront Drive East', '=>', 'Waterfront Drive East')\n",
      "('William Penn Highway', '=>', 'William Penn Highway')\n",
      "('Perry Highway', '=>', 'Perry Highway')\n",
      "('Lincoln Highway', '=>', 'Lincoln Highway')\n",
      "('Tri-Boro Expressway', '=>', 'Tri-Boro Expressway')\n",
      "('The Maples', '=>', 'The Maples')\n",
      "('Business Route 22', '=>', 'Business Route 22')\n",
      "('California Automotive', '=>', 'California Automotive')\n",
      "('Arlington Park', '=>', 'Arlington Park')\n",
      "('Windgap Ave Bldg', '=>', 'Windgap Ave Bldg')\n",
      "('Washington Pl', '=>', 'Washington Place')\n",
      "('Montgomery Pl', '=>', 'Montgomery Place')\n",
      "('Harding', '=>', 'Harding')\n",
      "('Selvin Dr', '=>', 'Selvin Drive')\n",
      "('Clubhouse Dr', '=>', 'Clubhouse Drive')\n",
      "('Eastminster Dr', '=>', 'Eastminster Drive')\n",
      "('Washington Pike', '=>', 'Washington Pike')\n",
      "('Steubenville Pike', '=>', 'Steubenville Pike')\n",
      "('Old Washington Pike', '=>', 'Old Washington Pike')\n",
      "('Greensburg Pike', '=>', 'Greensburg Pike')\n",
      "('Kittanning Pike', '=>', 'Kittanning Pike')\n",
      "('4th Ave.', '=>', '4th Ave.')\n",
      "('Murray Ave.', '=>', 'Murray Ave.')\n",
      "('Chartiers Ave.', '=>', 'Chartiers Ave.')\n",
      "('Crafton Ingram Shopping Center', '=>', 'Crafton Ingram Shopping Center')\n",
      "('Patton Street Extension', '=>', 'Patton Street Extension')\n",
      "('Thomas Street Extension', '=>', 'Thomas Street Extension')\n",
      "('Virginia Avenue Extension', '=>', 'Virginia Avenue Extension')\n",
      "('Broadway Avenue Extension', '=>', 'Broadway Avenue Extension')\n",
      "('Mount Troy Road Extension', '=>', 'Mount Troy Road Extension')\n",
      "('Federal Street Extension', '=>', 'Federal Street Extension')\n",
      "('Kennedy', '=>', 'Kennedy')\n",
      "('Elmer L Williams Sq', '=>', 'Elmer L Williams Sq')\n",
      "('Harvard Sq', '=>', 'Harvard Sq')\n",
      "('Mckees Rocks Plaza', '=>', 'Mckees Rocks Plaza')\n",
      "('Penn Plaza', '=>', 'Penn Plaza')\n",
      "('South Dithridge St', '=>', 'South Dithridge Street')\n",
      "('South Craig St', '=>', 'South Craig Street')\n",
      "('West St', '=>', 'West Street')\n",
      "('Walnut St', '=>', 'Walnut Street')\n",
      "('S Graham St', '=>', 'S Graham Street')\n",
      "('7th St', '=>', '7th Street')\n",
      "('Aster St', '=>', 'Aster Street')\n",
      "('Stanwix St', '=>', 'Streetanwix Street')\n",
      "('Mary St', '=>', 'Mary Street')\n",
      "('Hemlock St', '=>', 'Hemlock Street')\n",
      "('Winthrop St', '=>', 'Winthrop Street')\n",
      "('Copeland St', '=>', 'Copeland Street')\n",
      "('S 22nd St', '=>', 'S 22nd Street')\n",
      "('Henry St', '=>', 'Henry Street')\n",
      "('S Craig St', '=>', 'S Craig Street')\n",
      "('N Neville St', '=>', 'N Neville Street')\n",
      "('James St', '=>', 'James Street')\n",
      "('Mirror St', '=>', 'Mirror Street')\n",
      "('North Bellefield St', '=>', 'North Bellefield Street')\n",
      "('Railroad St', '=>', 'Railroad Street')\n",
      "('Castleman St', '=>', 'Castleman Street')\n",
      "('Saline St', '=>', 'Saline Street')\n",
      "('S Whitfield St', '=>', 'S Whitfield Street')\n",
      "('N Craig St', '=>', 'N Craig Street')\n",
      "('Bellefonte St', '=>', 'Bellefonte Street')\n",
      "('Monroeville Mall', '=>', 'Monroeville Mall')\n",
      "('Maple Street Extended', '=>', 'Maple Street Extended')\n",
      "('Long Valley Drive Extended', '=>', 'Long Valley Drive Extended')\n",
      "('Thomas Street Extended', '=>', 'Thomas Street Extended')\n",
      "('Montour Street Extended', '=>', 'Montour Street Extended')\n",
      "('Chartiers Avenue Extended', '=>', 'Chartiers Avenue Extended')\n",
      "('Aiken Road Extended', '=>', 'Aiken Road Extended')\n",
      "('Grandview Drive North', '=>', 'Grandview Drive North')\n",
      "('Grandview Drive South', '=>', 'Grandview Drive South')\n",
      "('Penn Circle South', '=>', 'Penn Circle South')\n",
      "('48', '=>', '48')\n",
      "('Grant Ave Ext', '=>', 'Grant Ave Ext')\n",
      "('Delmar Alley', '=>', 'Delmar Alley')\n",
      "('Oak Alley', '=>', 'Oak Alley')\n",
      "('Park Alley', '=>', 'Park Alley')\n",
      "('Clay Alley', '=>', 'Clay Alley')\n",
      "('Vine Alley', '=>', 'Vine Alley')\n",
      "('McGuinness Alley', '=>', 'McGuinness Alley')\n",
      "('howe', '=>', 'howe')\n",
      "('Washington Blvd', '=>', 'Washington Boulevard')\n",
      "('Clairton Blvd', '=>', 'Clairton Boulevard')\n",
      "('Beechwood Blvd', '=>', 'Beechwood Boulevard')\n",
      "('Fort Duquesne Blvd', '=>', 'Fort Duquesne Boulevard')\n",
      "('Pennsbury Blvd', '=>', 'Pennsbury Boulevard')\n",
      "('W Liberty Ave', '=>', 'W Liberty Avenue')\n",
      "('Forbes Ave', '=>', 'Forbes Avenue')\n",
      "('South Aiken Ave', '=>', 'South Aiken Avenue')\n",
      "('Highland Ave', '=>', 'Highland Avenue')\n",
      "('Morewood Ave', '=>', 'Morewood Avenue')\n",
      "('S Negley Ave', '=>', 'S Negley Avenue')\n",
      "('Liberty Ave', '=>', 'Liberty Avenue')\n",
      "('Shadeland Ave', '=>', 'Shadeland Avenue')\n",
      "('Penn Ave', '=>', 'Penn Avenue')\n",
      "('Centre Ave', '=>', 'Centre Avenue')\n",
      "('S Millvale Ave', '=>', 'S Millvale Avenue')\n",
      "('1st Ave', '=>', '1st Avenue')\n",
      "('Greydon Ave', '=>', 'Greydon Avenue')\n",
      "('Elizabeth Ave', '=>', 'Elizabeth Avenue')\n",
      "('Lynnwood Ave', '=>', 'Lynnwood Avenue')\n",
      "('Fifth Ave', '=>', 'Fifth Avenue')\n",
      "('Arlington Ave', '=>', 'Arlington Avenue')\n",
      "('Shady Ave', '=>', 'Shady Avenue')\n",
      "('Friendship Ave', '=>', 'Friendship Avenue')\n",
      "('S. Aiken Ave', '=>', 'S. Aiken Avenue')\n",
      "('5th Ave', '=>', '5th Avenue')\n",
      "('E Warrington Ave', '=>', 'E Warrington Avenue')\n",
      "('Faber Ter', '=>', 'Faber Terrece')\n",
      "('Colby Ter', '=>', 'Colby Terrece')\n",
      "('Coles Row', '=>', 'Coles Row')\n"
     ]
    }
   ],
   "source": [
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"Ter\" : \"Terrece\",\n",
    "            \"Pl\" : \"Place\",\n",
    "           \"Sq\" : \"Square\"\n",
    "            }\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    m = street_type_re.search(name)\n",
    "    if m.group() not in expected:\n",
    "        if m.group() in mapping.keys():\n",
    "            name = re.sub(m.group(), mapping[m.group()], name)\n",
    "    return name\n",
    "\n",
    "def test():\n",
    "    for st_type, ways in st_types.items():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print (name, \"=>\", better_name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "000ed3e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'15': set(['15017',\n",
      "            '15031',\n",
      "            '15034',\n",
      "            '15035',\n",
      "            '15057',\n",
      "            '15064',\n",
      "            '15102',\n",
      "            '15104',\n",
      "            '15106',\n",
      "            '15108',\n",
      "            '15110',\n",
      "            '15112',\n",
      "            '15120',\n",
      "            '15120-5000',\n",
      "            '15120-9998',\n",
      "            '15122',\n",
      "            '15132',\n",
      "            '15136',\n",
      "            '151363',\n",
      "            '15137',\n",
      "            '15142',\n",
      "            '15145',\n",
      "            '15146',\n",
      "            '15147',\n",
      "            '15148',\n",
      "            '15201',\n",
      "            '15202',\n",
      "            '15203',\n",
      "            '15203-2275',\n",
      "            '15204',\n",
      "            '15205',\n",
      "            '15206',\n",
      "            '15206-2011',\n",
      "            '15206-2032',\n",
      "            '15206-2033',\n",
      "            '15206-2038',\n",
      "            '15206-2058',\n",
      "            '15206-2396',\n",
      "            '15206-2696',\n",
      "            '15206-2697',\n",
      "            '15206-3807',\n",
      "            '15206-4320',\n",
      "            '15206-4336',\n",
      "            '15206-4403',\n",
      "            '15206-4449',\n",
      "            '15206-4456',\n",
      "            '15206-4471',\n",
      "            '15206-4472',\n",
      "            '15206-4818',\n",
      "            '15206-5311',\n",
      "            '15207',\n",
      "            '15208',\n",
      "            '15209',\n",
      "            '15210',\n",
      "            '15210-1845',\n",
      "            '15211',\n",
      "            '15212',\n",
      "            '15213',\n",
      "            '15213-1400',\n",
      "            '15213-1405',\n",
      "            '15213-1503',\n",
      "            '15213-1678',\n",
      "            '15213-1704',\n",
      "            '15213-1705',\n",
      "            '15213-1713',\n",
      "            '15213-1738',\n",
      "            '15213-1763',\n",
      "            '15213-2608',\n",
      "            '15213-2712',\n",
      "            '15213-2909',\n",
      "            '15213-2911',\n",
      "            '15213-3704',\n",
      "            '15213-4026',\n",
      "            '15214',\n",
      "            '15215',\n",
      "            '15216',\n",
      "            '15217',\n",
      "            '15218',\n",
      "            '15219',\n",
      "            '15220',\n",
      "            '15221',\n",
      "            '15222',\n",
      "            '15223',\n",
      "            '15224',\n",
      "            '15225',\n",
      "            '15226',\n",
      "            '15227',\n",
      "            '15228',\n",
      "            '15229',\n",
      "            '15232',\n",
      "            '15232-1418',\n",
      "            '15232-1419',\n",
      "            '15232-1421',\n",
      "            '15232-1447',\n",
      "            '15232-1803',\n",
      "            '15232-1823',\n",
      "            '15232-1826',\n",
      "            '15232-1828',\n",
      "            '15232-1832',\n",
      "            '15232-1833',\n",
      "            '15232-1845',\n",
      "            '15232-1879',\n",
      "            '15232-2106',\n",
      "            '15232-2131',\n",
      "            '15232-2210',\n",
      "            '15232-2705',\n",
      "            '15232-2716',\n",
      "            '15232-2734',\n",
      "            '15232-2735',\n",
      "            '15233',\n",
      "            '15234',\n",
      "            '15235',\n",
      "            '15236',\n",
      "            '15237',\n",
      "            '15238',\n",
      "            '15239',\n",
      "            '15240',\n",
      "            '15241',\n",
      "            '15243',\n",
      "            '15244',\n",
      "            '15247',\n",
      "            '15260',\n",
      "            '15261',\n",
      "            '15290']),\n",
      " '16': set(['16063'])}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code checks for zipcode whether they begin with '94' or '95' or something else\n",
    "'''\n",
    "OSMFILE = \"Pittsburgh.osm\"\n",
    "# OSMFILE = \"sample.osm\" #This is here so I can switch back and forth between the full .osm file and the sample\n",
    "zip_type_re = re.compile(r'\\d{5}$')\n",
    "\n",
    "def audit_ziptype(zip_types, zipcode):\n",
    "    if zipcode[0:2] != 15:\n",
    "        zip_types[zipcode[0:2]].add(zipcode)\n",
    "    elif zipcode[0:2] != 16:\n",
    "        zip_types[zipcode[0:2]].add(zipcode)\n",
    "        \n",
    "def is_zipcode(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "def audit_zip(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    zip_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_zipcode(tag):\n",
    "                    audit_ziptype(zip_types,tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return zip_types\n",
    "\n",
    "zip_print = audit_zip(OSMFILE)\n",
    "\n",
    "def test():    \n",
    "    pprint.pprint(dict(zip_print))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6893359f",
   "metadata": {},
   "source": [
    "# This Code Will Update Non 5-digit Zipcodes.\n",
    "* If it is a 8 or 9 digit only the first 5 digits will be kept.\n",
    "* If it has the state name in front, only the 5 digits are kept.\n",
    "* If it is something else, will not change anything as it might result in error when validating the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07c44109",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15206-4320 => 15206\n",
      "15205 => 15205\n",
      "15204 => 15204\n",
      "15207 => 15207\n",
      "15206 => 15206\n",
      "15201 => 15201\n",
      "15203 => 15203\n",
      "15202 => 15202\n",
      "15209 => 15209\n",
      "15208 => 15208\n",
      "15213-1705 => 15213\n",
      "15213-2608 => 15213\n",
      "15206-4449 => 15206\n",
      "15232-1845 => 15232\n",
      "15213-1763 => 15213\n",
      "15206-5311 => 15206\n",
      "15213-3704 => 15213\n",
      "15110 => 15110\n",
      "15290 => 15290\n",
      "15112 => 15112\n",
      "15216 => 15216\n",
      "15217 => 15217\n",
      "15214 => 15214\n",
      "15215 => 15215\n",
      "15212 => 15212\n",
      "15213 => 15213\n",
      "15210 => 15210\n",
      "15211 => 15211\n",
      "15057 => 15057\n",
      "15218 => 15218\n",
      "15219 => 15219\n",
      "15203-2275 => 15203\n",
      "15213-1678 => 15213\n",
      "15210-1845 => 15210\n",
      "15206-4471 => 15206\n",
      "15206-4472 => 15206\n",
      "15232-2210 => 15232\n",
      "15206-2038 => 15206\n",
      "15213-2712 => 15213\n",
      "15213-1503 => 15213\n",
      "15206-3807 => 15206\n",
      "15108 => 15108\n",
      "15106 => 15106\n",
      "15104 => 15104\n",
      "15102 => 15102\n",
      "15223 => 15223\n",
      "15222 => 15222\n",
      "15221 => 15221\n",
      "15220 => 15220\n",
      "15227 => 15227\n",
      "15226 => 15226\n",
      "15225 => 15225\n",
      "15224 => 15224\n",
      "15229 => 15229\n",
      "15228 => 15228\n",
      "15206-2032 => 15206\n",
      "151363 => 151363\n",
      "15232-1879 => 15232\n",
      "15213-4026 => 15213\n",
      "15232-1418 => 15232\n",
      "15232-1419 => 15232\n",
      "15132 => 15132\n",
      "15232-2734 => 15232\n",
      "15232-2735 => 15232\n",
      "15137 => 15137\n",
      "15136 => 15136\n",
      "15232-1421 => 15232\n",
      "15238 => 15238\n",
      "15239 => 15239\n",
      "15234 => 15234\n",
      "15235 => 15235\n",
      "15236 => 15236\n",
      "15031 => 15031\n",
      "15034 => 15034\n",
      "15233 => 15233\n",
      "15206-2396 => 15206\n",
      "15120 => 15120\n",
      "15122 => 15122\n",
      "15213-1405 => 15213\n",
      "15213-1400 => 15213\n",
      "15247 => 15247\n",
      "15206-4818 => 15206\n",
      "15241 => 15241\n",
      "15240 => 15240\n",
      "15243 => 15243\n",
      "15244 => 15244\n",
      "15206-4403 => 15206\n",
      "15213-1704 => 15213\n",
      "15232-1803 => 15232\n",
      "15213-2909 => 15213\n",
      "15232-2716 => 15232\n",
      "15017 => 15017\n",
      "15232-2131 => 15232\n",
      "15232-1447 => 15232\n",
      "15232-1832 => 15232\n",
      "15232-1833 => 15232\n",
      "15213-2911 => 15213\n",
      "15213-1738 => 15213\n",
      "15206-2011 => 15206\n",
      "15120-5000 => 15120\n",
      "15142 => 15142\n",
      "15237 => 15237\n",
      "15146 => 15146\n",
      "15147 => 15147\n",
      "15145 => 15145\n",
      "15148 => 15148\n",
      "15232-2705 => 15232\n",
      "15232 => 15232\n",
      "15261 => 15261\n",
      "15260 => 15260\n",
      "15035 => 15035\n",
      "15064 => 15064\n",
      "15232-1828 => 15232\n",
      "15232-1826 => 15232\n",
      "15232-1823 => 15232\n",
      "15206-2058 => 15206\n",
      "15206-4336 => 15206\n",
      "15206-2696 => 15206\n",
      "15206-2697 => 15206\n",
      "15232-2106 => 15232\n",
      "15206-2033 => 15206\n",
      "15213-1713 => 15213\n",
      "15206-4456 => 15206\n",
      "15120-9998 => 15120\n",
      "16063 => 16063\n"
     ]
    }
   ],
   "source": [
    "def update_zipcode(zipcode):\n",
    "    if re.findall(r'^\\d{5}$', zipcode): # 5 digits 94140\n",
    "        valid_zipcode = zipcode\n",
    "        return valid_zipcode\n",
    "    elif re.findall(r'(^\\d{5})-\\d{3}$', zipcode): # 8 digits 94150-029\n",
    "        valid_zipcode = re.findall(r'(^\\d{5})-\\d{3}$', zipcode)[0]\n",
    "        return valid_zipcode\n",
    "    elif re.findall(r'(^\\d{5})-\\d{4}$', zipcode): # 9 digits 95130-0239\n",
    "        valid_zipcode = re.findall(r'(^\\d{5})-\\d{4}$', zipcode)[0]\n",
    "        return valid_zipcode\n",
    "    elif re.findall(r'CA\\s*\\d{5}', zipcode): # with state code CA 95130\n",
    "        valid_zipcode =re.findall(r'\\d{5}', zipcode)[0]  \n",
    "        return valid_zipcode  \n",
    "    else: #return default zipcode to avoid overwriting\n",
    "        return zipcode\n",
    "    \n",
    "def test_zip():\n",
    "    for zips, ways in zip_print.iteritems():\n",
    "        for name in ways:\n",
    "            better_name = update_zipcode(name)\n",
    "            print name, \"=>\", better_name\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_zip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aa3449",
   "metadata": {},
   "source": [
    "# Following Code is from Udacity Lesson 13 Scema.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c800a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import codecs\n",
    "import cerberus\n",
    "import schema\n",
    "\n",
    "# %load schema.py\n",
    "\n",
    "schema = {\n",
    "    'node': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'lat': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'lon': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'node_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'way_nodes': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'node_id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'position': {'required': True, 'type': 'integer', 'coerce': int}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde99ca6",
   "metadata": {},
   "source": [
    "# Following Code is from Udacity Lesson 13 Data.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf050fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "OSM_PATH = \"Pittsburgh.osm\"\n",
    "OSMFILE = \"Pittsburgh.osm\"\n",
    "\n",
    "#OSM_PATH = \"Sample.osm\"  #This is here so I can switch back and forth between the full .osm file and the sample code\n",
    "#OSMFILE = \"Sample.osm\"   #This is here so I can switch back and forth between the full .osm file and the sample code\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Circle\", \"Terrace\", \"Way\"]\n",
    "\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"Rd\": \"Road\"\n",
    "            }\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "    \n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  \n",
    "    p=0\n",
    "    \n",
    "    if element.tag == 'node':\n",
    "        for i in NODE_FIELDS:\n",
    "            node_attribs[i] = element.attrib[i]\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            node_tags_attribs = {}\n",
    "            temp = LOWER_COLON.search(tag.attrib['k'])\n",
    "            is_p = PROBLEMCHARS.search(tag.attrib['k'])\n",
    "            if is_p:\n",
    "                continue\n",
    "            elif temp:\n",
    "                split_char = temp.group(1)\n",
    "                split_index = tag.attrib['k'].index(split_char)\n",
    "                type1 = temp.group(1)\n",
    "                node_tags_attribs['id'] = element.attrib['id']\n",
    "                node_tags_attribs['key'] = tag.attrib['k'][split_index+2:]\n",
    "                node_tags_attribs['value'] = tag.attrib['v']\n",
    "                node_tags_attribs['type'] = tag.attrib['k'][:split_index+1]\n",
    "                if node_tags_attribs['type'] == \"addr\" and node_tags_attribs['key'] == \"street\":\n",
    "                    node_tags_attribs['value'] = update_name(tag.attrib['v'], mapping) \n",
    "            else:\n",
    "                node_tags_attribs['id'] = element.attrib['id']\n",
    "                node_tags_attribs['key'] = tag.attrib['k']\n",
    "                node_tags_attribs['value'] = tag.attrib['v']\n",
    "                node_tags_attribs['type'] = 'regular'\n",
    "                if node_tags_attribs['type'] == \"addr\" and node_tags_attribs['key'] == \"street\":\n",
    "                    node_tags_attribs['value'] = update_name(tag.attrib['v'], mapping) \n",
    "            tags.append(node_tags_attribs)\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        id = element.attrib['id']\n",
    "        for i in WAY_FIELDS:\n",
    "            way_attribs[i] = element.attrib[i]\n",
    "        for i in element.iter('nd'):\n",
    "            d = {}\n",
    "            d['id'] = id\n",
    "            d['node_id'] = i.attrib['ref']\n",
    "            d['position'] = p\n",
    "            p+=1\n",
    "            way_nodes.append(d)\n",
    "        for c in element.iter('tag'):\n",
    "            temp = LOWER_COLON.search(c.attrib['k'])\n",
    "            is_p = PROBLEMCHARS.search(c.attrib['k'])\n",
    "            e = {}\n",
    "            if is_p:\n",
    "                continue\n",
    "            elif temp:\n",
    "                split_char = temp.group(1)\n",
    "                split_index = c.attrib['k'].index(split_char)\n",
    "                e['id'] = id\n",
    "                e['key'] = c.attrib['k'][split_index+2:]\n",
    "                e['type'] = c.attrib['k'][:split_index+1]\n",
    "                e['value'] = c.attrib['v']\n",
    "                if e['type'] == \"addr\" and e['key'] == \"street\":\n",
    "                    e['value'] = update_name(c.attrib['v'], mapping) \n",
    "            else:\n",
    "                e['id'] = id\n",
    "                e['key'] = c.attrib['k']\n",
    "                e['type'] = 'regular'\n",
    "                e['value'] =  c.attrib['v']\n",
    "                if e['type'] == \"addr\" and e['key'] == \"street\":\n",
    "                    e['value'] = update_name(c.attrib['v'], mapping) \n",
    "            tags.append(e)\n",
    "        \n",
    "    return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "        \n",
    "    if element.tag == 'node':\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "    \n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "    with codecs.open(NODES_PATH, 'wb') as nodes_file, \\\n",
    "        codecs.open(NODE_TAGS_PATH, 'wb') as nodes_tags_file, \\\n",
    "        codecs.open(WAYS_PATH, 'wb') as ways_file, \\\n",
    "        codecs.open(WAY_NODES_PATH, 'wb') as way_nodes_file, \\\n",
    "        codecs.open(WAY_TAGS_PATH, 'wb') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_map(OSM_PATH, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0447b14",
   "metadata": {},
   "source": [
    "# Create DB and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b47c7b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('C750.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d33251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.text_factory = str\n",
    "cur = conn.cursor()\n",
    "\n",
    "#Make new tables\n",
    "cur.execute('''DROP TABLE IF EXISTS nodes''')\n",
    "cur.execute('''DROP TABLE IF EXISTS nodes_tags''')\n",
    "cur.execute('''DROP TABLE IF EXISTS ways''')\n",
    "cur.execute('''DROP TABLE IF EXISTS ways_tags''')\n",
    "cur.execute('''DROP TABLE IF EXISTS ways_nodes''')\n",
    "\n",
    "\n",
    "cur.execute('''CREATE TABLE nodes (\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    lat REAL,\n",
    "    lon REAL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT)\n",
    "''')\n",
    "\n",
    "with open('nodes.csv','r') as nodes_table:\n",
    "    dr = csv.DictReader(nodes_table)\n",
    "    to_db = [(i['id'], i['lat'], i['lon'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes VALUES (?, ?, ?, ?, ?, ?, ?, ?);\", to_db)\n",
    "\n",
    "\n",
    "cur.execute('''CREATE TABLE nodes_tags (\n",
    "    id INTEGER,\n",
    "    key TEXT,\n",
    "    value TEXT,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES nodes(id))\n",
    "''')\n",
    "\n",
    "with open('nodes_tags.csv','r') as nodes_tags_table:\n",
    "    dr = csv.DictReader(nodes_tags_table)\n",
    "    to_db = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes_tags VALUES (?, ?, ?, ?);\", to_db)\n",
    "\n",
    "cur.execute('''CREATE TABLE ways (\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version TEXT,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT)\n",
    "''')\n",
    "\n",
    "with open('ways.csv','r') as ways_table:\n",
    "    dr = csv.DictReader(ways_table)\n",
    "    to_db = [(i['id'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways VALUES (?, ?, ?, ?, ?, ?);\", to_db)\n",
    "\n",
    "cur.execute('''CREATE TABLE ways_tags (\n",
    "    id INTEGER NOT NULL,\n",
    "    key TEXT NOT NULL,\n",
    "    value TEXT NOT NULL,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id))\n",
    "''')\n",
    "\n",
    "with open('ways_tags.csv','r') as ways_tags_table:\n",
    "    dr = csv.DictReader(ways_tags_table)\n",
    "    to_db = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_tags VALUES (?, ?, ?, ?);\", to_db)\n",
    "\n",
    "cur.execute('''CREATE TABLE ways_nodes (\n",
    "    id INTEGER NOT NULL,\n",
    "    node_id INTEGER NOT NULL,\n",
    "    position INTEGER NOT NULL,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id),\n",
    "    FOREIGN KEY (node_id) REFERENCES nodes(id))\n",
    "''')\n",
    "\n",
    "with open('ways_nodes.csv','r') as ways_nodes_table:\n",
    "    dr = csv.DictReader(ways_nodes_table)\n",
    "    to_db = [(i['id'], i['node_id'], i['position']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_nodes VALUES (?, ?, ?);\", to_db)\n",
    "\n",
    "#Save changes to DB\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bab864",
   "metadata": {},
   "source": [
    "# Show File size of Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "124ddad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Pittsburgh.osm file is 435.068972 MB\n",
      "The Sample.osm file is 44.136175 MB\n",
      "The nodes.csv file is 161.025194 MB\n",
      "The nodes_tags.csv file is 3.648402 MB\n",
      "The ways.csv file is 20.26941 MB\n",
      "The ways_nodes.csv file is 54.190914 MB\n",
      "The ways_tags.csv file is 24.593397 MB\n",
      "The C750.sqlite file is 227.254272 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "print ('The Pittsburgh.osm file is {} MB'.format(os.path.getsize('Pittsburgh.osm')/1.0e6))\n",
    "print ('The Sample.osm file is {} MB'.format(os.path.getsize('Sample.osm')/1.0e6))\n",
    "print ('The nodes.csv file is {} MB'.format(os.path.getsize('nodes.csv')/1.0e6))\n",
    "print ('The nodes_tags.csv file is {} MB'.format(os.path.getsize('nodes_tags.csv')/1.0e6))\n",
    "print ('The ways.csv file is {} MB'.format(os.path.getsize('ways.csv')/1.0e6))\n",
    "print ('The ways_nodes.csv file is {} MB'.format(os.path.getsize('ways_nodes.csv')/1.0e6))\n",
    "print ('The ways_tags.csv file is {} MB'.format(os.path.getsize('ways_tags.csv')/1.0e6))\n",
    "print ('The C750.sqlite file is {} MB'.format(os.path.getsize('C750.sqlite')/1.0e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f63702",
   "metadata": {},
   "source": [
    "# Number of Ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2b5459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(323896,)\n"
     ]
    }
   ],
   "source": [
    "way_count = conn.cursor()\n",
    "way_count.execute(\"SELECT COUNT(*) FROM ways\")\n",
    "\n",
    "way_count = way_count.fetchall()\n",
    " \n",
    "for row in way_count:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ae765a",
   "metadata": {},
   "source": [
    "# Number of Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9004561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1860250,)\n"
     ]
    }
   ],
   "source": [
    "node_count = conn.cursor()\n",
    "node_count.execute(\"SELECT COUNT(*) FROM nodes\")\n",
    "\n",
    "node_count = node_count.fetchall()\n",
    " \n",
    "for row in node_count:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fc42de",
   "metadata": {},
   "source": [
    "\n",
    "# Unique Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "06c64c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1553,)\n"
     ]
    }
   ],
   "source": [
    "Total_Unique_users = conn.cursor()\n",
    "Total_Unique_users.execute(\"SELECT COUNT(DISTINCT(e.uid)) FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) e\")\n",
    "\n",
    "Total_Unique_users = Total_Unique_users.fetchall()\n",
    " \n",
    "for row in Total_Unique_users:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e4eb349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GeoKitten_import', 403814)\n",
      "('Karthoo_import', 324888)\n",
      "('Omnific', 249585)\n",
      "('cowdog', 116924)\n",
      "('DonovanG', 111671)\n",
      "('thetornado76', 100752)\n",
      "('abbafei', 83396)\n",
      "('GeoKitten', 82133)\n",
      "('Yaten_import', 67002)\n",
      "('tmb926', 66594)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "top_users = conn.cursor()\n",
    "top_users.execute(\"SELECT e.user, COUNT(*) as num FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e GROUP BY e.user ORDER BY num DESC LIMIT 10\")\n",
    "\n",
    "top_users = top_users.fetchall()\n",
    " \n",
    "for row in top_users:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e18242b",
   "metadata": {},
   "source": [
    "# Count of Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a5ac33e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('restaurant', 473)\n",
      "('place_of_worship', 328)\n",
      "('school', 257)\n",
      "('bench', 208)\n",
      "('waste_basket', 185)\n",
      "('fast_food', 138)\n",
      "('post_box', 135)\n",
      "('parking_entrance', 129)\n",
      "('library', 115)\n",
      "('cafe', 101)\n"
     ]
    }
   ],
   "source": [
    "amenities = conn.cursor()\n",
    "t = ('amenity',)\n",
    "amenities.execute(\"SELECT value, COUNT(*) as num FROM nodes_tags WHERE key=? GROUP BY value ORDER BY num DESC LIMIT 10\", t)\n",
    "\n",
    "amenities = amenities.fetchall()\n",
    " \n",
    "for row in amenities:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b516f7",
   "metadata": {},
   "source": [
    "# Count of Tourism Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "505e4d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('artwork', 37)\n",
      "('information', 22)\n",
      "('hotel', 16)\n",
      "('viewpoint', 16)\n",
      "('museum', 9)\n",
      "('picnic_site', 5)\n",
      "('attraction', 3)\n",
      "('gallery', 2)\n",
      "('amusement_ride', 1)\n",
      "('guest_house', 1)\n",
      "('hostel', 1)\n",
      "('motel', 1)\n"
     ]
    }
   ],
   "source": [
    "tourism = conn.cursor()\n",
    "t = ('tourism', 'tourism_1')\n",
    "tourism.execute(\"SELECT value, COUNT(*) as num FROM nodes_tags WHERE key=? GROUP BY value UNION SELECT value, COUNT(*) as num FROM nodes_tags WHERE key=? GROUP BY value ORDER BY num DESC\", t)\n",
    "\n",
    "tourism = tourism.fetchall()\n",
    " \n",
    "for row in tourism:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2020d25b",
   "metadata": {},
   "source": [
    "# Top Count of Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cddda10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pittsburgh', 1487)\n",
      "('North Versailles', 51)\n",
      "('Penn Hills', 44)\n",
      "('McKees Rocks', 36)\n",
      "('Bridgeville', 26)\n",
      "('Turtle Creek', 18)\n",
      "('Carnegie', 14)\n",
      "('Forest Hills', 13)\n",
      "('Verona', 11)\n",
      "('Aspinwall', 11)\n"
     ]
    }
   ],
   "source": [
    "top_city = conn.cursor()\n",
    "t = ('city',)\n",
    "top_city.execute(\"SELECT value, COUNT(*) as num FROM nodes_tags WHERE key=? GROUP BY value ORDER BY num DESC LIMIT 10\", t)\n",
    "\n",
    "top_city = top_city.fetchall()\n",
    " \n",
    "for row in top_city:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59767fa1",
   "metadata": {},
   "source": [
    "# Analysis/Suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5cd219",
   "metadata": {},
   "source": [
    "When I was looking over the Postal codes I was surprised at how well the postal codes stuck to either 5 or 10 digit postal codes.  I was expecting to see multiple inconsistencies in how the postal codes were formatted.  This made for an easy standardization of the postal codes during clean up. It would be nice if there was a way to standardize street names as these fields are entered inconsistently.  However with OSM being an open source project I think this would cost a considerable amount of money in man hours or other resources to be able to standardize the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc049fe1",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108eec7e",
   "metadata": {},
   "source": [
    "This project was a great way to discover how people can join together to make data that can be used in a multitude of ways.  \n",
    "While the data has its flaws it is able to be cleaned up and used in a variety of ways.  While it doesn't have the power behind such of google maps I can see OSM being a powerful tool for people looking for a free, open source area of information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
